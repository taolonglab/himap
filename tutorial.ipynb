{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiMAP tutorial: human data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you've installed HiMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test, download files from 2 samples of DIABIMMUNE Tri-county cohort:\n",
    "* [Sample G3225 (forward)](https://pubs.broadinstitute.org/diabimmune/data/10/G63225_R1_001.fastq.gz)\n",
    "* [Sample G3225 (reverse)](https://pubs.broadinstitute.org/diabimmune/data/10/G63225_R2_001.fastq.gz)\n",
    "* [Sample G3228 (forward)](https://pubs.broadinstitute.org/diabimmune/data/10/G63228_R1_001.fastq.gz)\n",
    "* [Sample G3228 (reverse)](https://pubs.broadinstitute.org/diabimmune/data/10/G63228_R2_001.fastq.gz)\n",
    "\n",
    "and save them to a folder. For the purposes of this tutorial, we will save them to `~/data/diabimmune/fastq_tutorial/`. In these samples, V4 hyper-variable region is sequenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(himap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files and specifying folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need full paths to the FASTQ files for pair-end 16S reads. Filenames for all forwards reads will be stored in `fq_fwd` and reverse in `fq_rev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fastq_path = path.expand('~/data/diabimmune/tutorial/fastq')\n",
    "fq_fwd = read_files(fastq_path, 'R1')\n",
    "fq_rev = read_files(fastq_path, 'R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filenames are used to extract identifiers for each file, either with forward or reverse reads. The function `sampleids_from_filenames` will retrieve those, given a separator (default is the underscore _). For example, from forward files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(fq_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would like `G63225` and `G63228` labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ids = sampleids_from_filenames(fq_fwd, separator='_')\n",
    "head(sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HiMAP will output a number of output files and folders, which will be explained as we go along, so specify a folder where to save all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = path.expand('~/data/diabimmune/tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First specify the output files for each sample, the merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fq_mer = file.path(out_path, 'merged', paste0(sample_ids, '.fastq'))\n",
    "mergestats = merge_pairs(fq_fwd, fq_rev, fq_mer, verbose=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many reads are merged and not merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames(mergestats) = sample_ids\n",
    "mergestats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 107,153 total reads 1,264 were not merged due to low percentage similarity in the alignment (if this number is high, consider lowering `min_sim` parameter) or too high alignment length (if this number is too high, consider lowering `low_aln_len`). See `?merge_pairs` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim PCR primers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim PCR primers from merged reads. Specify region 'V4' or 'V3-V4'. First, create output filenames, then do the trimming, using V4 region primers. Exact primer sequences can also be specified using arguments `pr_fwd` and `pr_rev`; see `?pcr_primer_trimmer` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fq_tri = file.path(out_path, 'trimmed', paste0(sample_ids, '.fastq'))\n",
    "trimstats = pcr_primer_trimmer(fq_mer, fq_tri, region='V4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames(trimstats) = sample_ids\n",
    "trimstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No primers were found in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control and fixed-length trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input for DADA2 denoising requires all sequences to be trimmed to the fixed length, we first inspect the distribution of sequence lengths before choosing this parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqlen.ft = sequence_length_table(fq_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3) # Make a smaller plot\n",
    "plot(seqlen.ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the minimum trimming length by finding the length above which we have 99% of the reads, since most of the reads here have 252 and 253 nt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trim_length = ftquantile(seqlen.ft, 0.01)\n",
    "trim_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use that to do the trimming. For quality control keep sequences with 2 or less expected errors [Edgar, Flyberg]. This also removes any phiX sequences and sequences containing any Ns after truncation (trimming):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fq_fil = file.path(out_path, 'filtered', paste0(sample_ids, '.fastq'))\n",
    "filtstats = dada2::filterAndTrim(fq_tri, fq_fil, truncLen=trim_length, maxEE=2, multithread=T, verbose=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the function returns the table showing the number of reads kept during the QC filtering process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For denoising, we use `dada` function from the dada2 package. After the denoising part is done, for each partition, we retrieve pre-trimmed sequences and add their consensus sequence back. This can improve the accuracy of the alignment to the reference database in one of the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dada_result = dada_denoise(fq_fil, fq_tri)\n",
    "dada_result = readRDS(file.path(out_path, 'dada_result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saveRDS(dada_result, file.path(out_path, 'dada_result'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count sequences, remove bimeras, collapse sequences from multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we count the number of reads for each denoised sequence, remove chimeric (here bimeric) reads. If we work with multiple samples, we pool together denoised sequences that are identical up to shifts or length variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ab.dt = sequence_abundance(dada_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(ab.dt, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align sequences vs reference database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a final list of sequences, we align these sequences to the reference database. We BLAST these sequences against our V4 region HiMAP database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_output_file = file.path(out_path, 'blast_vs_db.txt')\n",
    "blast_output = blast(ab.dt, blast_output_file, region='V4', max_target_seqs=100, verbose=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate OSU abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use sequence abundance table `ab.dt` and `blast_output` to infer OSUs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osu_ab.dt = abundance(ab.dt, blast_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(osu_ab.dt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_table(osu_ab.dt, file.path(out_path, 'osu_abundances.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each OSU, we get taxonomic ranks for all detected strains, from NCBI Taxonomy database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osu_tax.dt = taxonomy(osu_ab.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(osu_tax.dt, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}